# Transformers United V2
## Stanford CS25 Winter 2023
### 1. Introduction

#### Readings
[Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)  
[The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)  
[The Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/)  

#### History

##### RNNs
- Seq2Seq
- LSTM
- GRU
-...

Did not work well for
- Long sequences
- Context

##### 2021
- solving long sequence problems such as protein folding
- few-shot / zero-shot generalisations
- multimodal, image generation from language DALL-E

##### 2022
- Reasoning capabilities
- Human alignment
- Control bias

#### Attention Is All You Need

Where did the transformer come from ?

[A Neural Probabilistic Language Model - 2003](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)  
[Sequence to Sequence Learning with Neural Networks - 2014](https://arxiv.org/pdf/1409.3215.pdf)  
[Neural Machine Translation by Jointly Learning to Align and Translate - 2014](https://arxiv.org/pdf/1409.0473.pdf)  



