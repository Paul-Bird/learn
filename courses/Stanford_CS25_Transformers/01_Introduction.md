# Transformers United V2
## Stanford CS25 Winter 2023
### 1. Introduction

#### Readings
[Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)  
[The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)  
[The Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/)  

#### History

##### RNNs
- Seq2Seq
- LSTM
- GRU
-...

Did not work well for
- Long sequences
- Context

##### 2021
- solving long sequence problems such as protein folding
- few-shot / zero-shot generalisations
- multimodal, image generation from language DALL-E

##### 2022
- Reasoning capabilities
- Human alignment
- Control bias

#### Attention Is All You Need

14:00